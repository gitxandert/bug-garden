# bug-garden
Processing and Max code for an audiovisual installation created in fall 2023

YouTube link to a demo and description of the installation: https://youtu.be/LKVbb8Y3uPg

This was my first audiovisual installation, which I created while a student at the Indiana University Jacobs School of Music. It was the culminating project for the Projects in Electronic Music course (MUS-K 506), which was a requirement for the Master's in Computer Music Composition (which I attained in December 2023, the same month that this installation premiered). This was the first installation of its kind in the history of the course, the degree, and the school. It features programming in Max, a visual programming environment for generating and manipulating music and audio, and in Processing, an integrated graphics library and programming environment based in Java. The Max patches and Processing sketches communicate with each other via the Open Sound Control (OSC) protocol and allow for an interactive experience in which users affect graphic objects on the screen to alter the soundscape produced by the Max patches.

The experience begins with a screen that is mostly black (representing darkness), except for a few large concentric circles in the upper-right corner of the screen (representing an area of light). Before the user interacts with the installation, eclectic shapes and visual patterns populate the darkness: these are the "bugs" that are roaming around, and which the user will try to catch and move into the light. While in the darkness, bugs will produce a particular type of noisy sound; when they have been successfully moved into the light, they not only change from abstract shapes and patterns into a fully-animated bug, but they also begin producing more tonal sounds. The orientation on screen, colors, and dimensions of a bug determine the quality and pitch of the sounds that they produce, and users interact with the installation by shifting these bugs around to create novel and variable soundscapes.

The Processing sketches rely on computer vision to facilitate this interactivity: users shine colored lights onto the computer camera, which materialize as small, colored circles of light. Each color serves a different function: a white light shone onto the screen will produce its own sound, which varies in spatialization and pitch according to the x and y positions of the light respectively; a green light will capture a bug caught in its aura and allow this bug to be moved from darkness to light and vice-versa; a blue light will change the color and dimensions of a bug it is shone upon, which will change the sound associated with that bug; and a red light will switch a bug it is shone upon with another that is roaming around in the darkness. As users walk into the installation, they are encouraged to pick a light and to scan a QR code associated with that light to open up a Google Doc that explains the functionality of that light, along with suggestions on how to use that light in creative conjunction with others.

The Processing code features object-oriented programming to render multiple instances of bugs onto a screen, along with blob detection to isolate the colored lights directed onto the camera and project these lights as colored circles onto the screen. This was my first project coding in Java; I learned the above concepts on the fly with a ton of help from the Coding Train tutorials on YouTube. (Thanks, Daniel!!) My improvement for the code would be to make it much more modular and reduce redundancy; my improvement for the installation would be to limit the amount of users who interact with the screen at once (too many lights creates too much confusion [and frustration]) and to use less-ambiguously-colored lights (which can get confused among each other, especially in close proximity). Overall, this was a major challenge and learning experience, but I am very grateful to have had the opportunity to have learned so many concepts and put them into practice in a fun, light-hearted way.
